{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConorD28/March-Madness/blob/main/March_Madness_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq6xsPJUvkXO",
        "outputId": "7a0b74dc-0338-428c-8dc7-58b8eebfd6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "inputs = pd.read_csv('MM inputs.csv')\n",
        "playoff_stats = pd.read_csv('Mar_Mad_playoffs.csv')\n",
        "\n",
        "print(inputs.isnull().sum().sum()) #Check if there are NA values\n",
        "print(playoff_stats.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9zr28DwjQ6D"
      },
      "source": [
        "\n",
        "\n",
        "# **Correlation/Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2LtJLaEVvqfY"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  cols = []\n",
        "  correlations = []\n",
        "  #corS = 0\n",
        "  if isinstance(target, np.ndarray):\n",
        "    target = pd.Series(target)\n",
        "  for col in dataset.columns:\n",
        "      #print(dataset.loc[:,col])\n",
        "      #print(col)\n",
        "      corS = dataset.loc[:,col].corr(target, method='spearman') # 'kendall'\n",
        "      corP = dataset.loc[:,col].corr(target)\n",
        "      if (abs(corP) > threshold) or (abs(corS) > threshold):\n",
        "        cor2 = max(abs(corP), abs(corS))\n",
        "        data.append(dataset.loc[:,col]) #make list of columns that meet the threshold\n",
        "        cols.append(col)\n",
        "        correlations.append(cor2) #make list of correlations that meet the threshold\n",
        "  if len(data) == 0:\n",
        "     return pd.DataFrame()\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "  df_len = len(df.columns)\n",
        "  df.insert(df_len, 'corrs', correlations)\n",
        "  df = df.sort_values(by=df.columns[-1], ascending=False, key = abs)\n",
        "  df = df.transpose()\n",
        "  df_corrs = df.iloc[-1:, :]\n",
        "  df = df.drop(df.tail(1).index)\n",
        "  return df, df_corrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngjdaqWIvssV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random.mtrand import random_sample\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, MultiTaskLassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5q3JY8L1e1M"
      },
      "outputs": [],
      "source": [
        "def Scores(y, y_pred):\n",
        "  MSE = mean_squared_error(y, y_pred)\n",
        "  MAE = mean_absolute_error(y, y_pred)\n",
        "\n",
        "  range_y = y.max() - y.min()\n",
        "  Normalized_RMSE = (np.sqrt(MSE)/abs(range_y))\n",
        "  Normalized_MAE = (MAE/abs(range_y))\n",
        "  #print(f'Normalized RMSE:{ Normalized_RMSE:.2f}')\n",
        "  #print(f'Normalized MAE:{ Normalized_MAE:.2f}')\n",
        "  #print(f'MAE:{ MAE:.3f}')\n",
        "  #print(f'RMSE:{ np.sqrt(MSE):.3f}')\n",
        "  return Normalized_RMSE, Normalized_MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifryB2OHrVml"
      },
      "outputs": [],
      "source": [
        "def Predict_Scores(model, X_tr, X_te, y_tr, y_te, t_sc):#, predict_df):\n",
        "  y_train_pred = model.predict(X_tr)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  if len(y_te) != 0:\n",
        "    y_test_pred = model.predict(X_te)\n",
        "  else:\n",
        "    y_test_pred = pd.DataFrame()\n",
        "  #print('y test values:')\n",
        "  #print(y_test_pred)\n",
        "\n",
        "  #print('Training Scores:')\n",
        "  NRMSE_tr, NMAE_tr = Scores(y_tr, y_train_pred)\n",
        "\n",
        "  #print('after inverse transform, training off by:')\n",
        "  y_train_pred_transformed = t_sc.inverse_transform(y_train_pred.reshape(-1, 1)) # Reshape y_train_pred\n",
        "  y_train_pred_transformed = pd.Series(y_train_pred_transformed.flatten())\n",
        "  y_tr_transformed = t_sc.inverse_transform(y_tr.values.reshape(-1, 1))\n",
        "  y_tr_transformed = pd.Series(y_tr_transformed.flatten())\n",
        "  inv_error_tr_transformed = np.abs(y_tr_transformed - y_train_pred_transformed)\n",
        "  #print('y training values:')\n",
        "  #print(y_train_pred_transformed)\n",
        "\n",
        "  error_test = 0\n",
        "  inv_error_test_transformed = 0\n",
        "  y_te_transformed = 0\n",
        "  y_test_pred_transformed = 0\n",
        "  #Test Predictions:\n",
        "  if len(y_te) != 0:\n",
        "    error_test = y_te - y_test_pred\n",
        "    y_te_transformed = t_sc.inverse_transform(y_te.reshape(-1, 1))\n",
        "    y_te_transformed = y_te_transformed.flatten()\n",
        "    y_test_pred_transformed = t_sc.inverse_transform(y_test_pred.reshape(-1, 1))\n",
        "    y_test_pred_transformed = y_test_pred_transformed.flatten()\n",
        "    y_te_transformed = t_sc.inverse_transform(y_te.reshape(-1, 1))\n",
        "    inv_error_test_transformed = np.abs(y_te_transformed - y_test_pred_transformed)\n",
        "    #print(y_test_pred_transformed)\n",
        "\n",
        "  #Predict:\n",
        "  #predictions = model.predict(predict_df)\n",
        "\n",
        "  return NRMSE_tr, NMAE_tr, error_test, inv_error_tr_transformed, inv_error_test_transformed, y_te, y_test_pred_transformed, y_train_pred_transformed#, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhvX9hZQi5U7"
      },
      "source": [
        "# **ML Tuning Algorithms**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna\n",
        "!pip install joblib\n",
        "import joblib\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "d3UKVx4RG1TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cvp4C7sqoRZx"
      },
      "outputs": [],
      "source": [
        "def Ridge_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "    def objective(trial, cv_runs, X_train, y_train):\n",
        "      alpha = trial.suggest_float(\"alpha\", 5, 20, log=True)#1e-4, 10.0; Alpha is the regularization strength\n",
        "      solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"])\n",
        "\n",
        "      model = Ridge(alpha=alpha, solver=solver, random_state=28)\n",
        "      score = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring=\"neg_mean_squared_error\").mean()\n",
        "      return -score  # Minimize the MSE\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"  Params: {trial.params}\")\n",
        "\n",
        "    best_model = Ridge(**trial.params, random_state=28)\n",
        "    best_model.fit(X_train, y_train)\n",
        "    joblib.dump(best_model, 'PPG_Ridge.pkl')\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O8UenllL9Tt"
      },
      "outputs": [],
      "source": [
        "def Lasso_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def Lasso_objective(trial, cv_runs, X_train, y_train):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-2, .09, log=True)##1e-4, 10.0; Regularization strength\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 1000, 10000, step=100)  # Max iterations\n",
        "    tol = trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True)  # Tolerance for stopping criteria\n",
        "\n",
        "    model = Lasso(alpha=alpha, max_iter=max_iter, tol=tol, random_state=28)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring=\"neg_mean_squared_error\").mean()\n",
        "    return -score  # Minimize the MSE\n",
        "\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(lambda trial: Lasso_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best trial:\")\n",
        "  trial = study.best_trial\n",
        "  print(f\"  Params: {trial.params}\")\n",
        "\n",
        "  best_model = Lasso(**trial.params, random_state=28)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  #joblib.dump(best_model, 'oPPG_Lasso.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arifqUP7P65I"
      },
      "outputs": [],
      "source": [
        "def Elastic_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def Elastic_objective(trial, cv_runs, X_train, y_train):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-2, .09, log=True)#1e-4, 10.0; Regularization strength\n",
        "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)  # Mixing ratio between Lasso and Ridge\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 1000, 10000, step=100)  # Max iterations\n",
        "    tol = trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True)  # Tolerance for stopping criteria\n",
        "\n",
        "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, tol=tol, random_state=28)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring=\"neg_mean_squared_error\").mean()\n",
        "    return -score  # Minimize the MSE\n",
        "\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(lambda trial: Elastic_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "\n",
        "  print(\"Best trial:\")\n",
        "  trial = study.best_trial\n",
        "  print(f\"  Params: {trial.params}\")\n",
        "\n",
        "  best_model = ElasticNet(**trial.params, random_state=28)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  #joblib.dump(best_model, 'PPG_Elastic.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFcBEcqFsezZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "def GBR_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def GBR_objective(trial, cv_runs, X_train, y_train):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True), #1e-3, 0.5\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "    }\n",
        "    model = GradientBoostingRegressor(**params, random_state=28)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring=\"neg_mean_squared_error\")\n",
        "    return -scores.mean()  # Minimize MSE\n",
        "\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(lambda trial: GBR_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "  best_model = GradientBoostingRegressor(**study.best_params, random_state=28)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  #joblib.dump(best_model, 'PPG_GBR.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYrV1hGVhgzH"
      },
      "outputs": [],
      "source": [
        "def BR_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def BR_objective(trial, cv_runs, X_train, y_train):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
        "    max_samples = trial.suggest_float('max_samples', 0.5, 1.0)\n",
        "    max_features = trial.suggest_float('max_features', 0.5, 1.0)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 30)  # For DecisionTreeRegressor\n",
        "\n",
        "    base_estimator = DecisionTreeRegressor(max_depth=max_depth, random_state=28)\n",
        "    model = BaggingRegressor(estimator=base_estimator, n_estimators=n_estimators,\n",
        "        max_samples=max_samples, max_features=max_features, random_state=28, n_jobs=-1)\n",
        "\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    mean_score = np.mean(scores)\n",
        "    return -mean_score\n",
        "\n",
        "  study = optuna.create_study(direction='minimize')\n",
        "  study.optimize(lambda trial: BR_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "  best_base_estimator = DecisionTreeRegressor(max_depth=study.best_params['max_depth'], random_state=28)\n",
        "  best_model = BaggingRegressor(estimator=best_base_estimator, n_estimators=study.best_params['n_estimators'],\n",
        "      max_samples=study.best_params['max_samples'], max_features=study.best_params['max_features'],\n",
        "      random_state=28, n_jobs=-1)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  #joblib.dump(best_model, 'PPG_BR.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49dqq-GiW8IS"
      },
      "outputs": [],
      "source": [
        "def SVR_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def linear_svr_objective(trial, cv_runs, X_train, y_train):\n",
        "    C = trial.suggest_float('C', 1e-4, .4, log=True) #1e-4, 1e2\n",
        "    epsilon = trial.suggest_float('epsilon', .1, 5.0, log=True)\n",
        "    model = LinearSVR(C=C, epsilon=epsilon, random_state=28, max_iter=100000)\n",
        "\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring='neg_mean_squared_error')\n",
        "    mean_score = np.mean(scores)\n",
        "    return -mean_score\n",
        "\n",
        "  study = optuna.create_study(direction='minimize')\n",
        "  study.optimize(lambda trial: linear_svr_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "  best_model = LinearSVR(C=study.best_params['C'], epsilon=study.best_params['epsilon'],\n",
        "                        random_state=28, max_iter=10000)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  #joblib.dump(best_model, 'PPG_SVR.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S79aLVgXqwdw"
      },
      "outputs": [],
      "source": [
        "def SVR_rbf_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def rbf_objective(trial, cv_runs, X_train, y_train):\n",
        "      C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
        "      gamma = trial.suggest_float('gamma', 1e-4, 1e1, log=True)\n",
        "      epsilon = trial.suggest_float('epsilon', 1e-4, 1e1, log=True)\n",
        "\n",
        "      model = SVR(kernel='rbf', C=C, gamma=gamma, epsilon=epsilon)\n",
        "      scores = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring='neg_mean_squared_error')\n",
        "      mean_score = np.mean(scores)\n",
        "      return -mean_score\n",
        "\n",
        "  study = optuna.create_study(direction='minimize')\n",
        "  study.optimize(lambda trial: rbf_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best parameters:\", study.best_params)\n",
        "  best_model = SVR(kernel='rbf', C=study.best_params['C'], gamma=study.best_params['gamma'],\n",
        "                 epsilon=study.best_params['epsilon'])\n",
        "  best_model.fit(X_train, y_train)\n",
        "  #joblib.dump(best_model, 'PPG_Rbf.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhMpaUciufvj"
      },
      "outputs": [],
      "source": [
        "def SVR_poly_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def poly_objective(trial, cv_runs, X_train, y_train):\n",
        "    C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
        "    gamma = trial.suggest_float('gamma', 1e-4, 1e1, log=True)\n",
        "    epsilon = trial.suggest_float('epsilon', 1e-4, 1e1, log=True)\n",
        "    degree = trial.suggest_int('degree', 2, 5)  # Degrees 2 through 5\n",
        "    coef0 = trial.suggest_float('coef0', 0.0, 10.0)  # Coefficient in kernel function\n",
        "\n",
        "    model = SVR(kernel='poly', C=C, gamma=gamma, epsilon=epsilon, degree=degree, coef0=coef0)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring='neg_mean_squared_error')\n",
        "    mean_score = np.mean(scores)\n",
        "    return -mean_score\n",
        "\n",
        "  study = optuna.create_study(direction='minimize')\n",
        "  study.optimize(lambda trial: poly_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "  best_model = SVR(kernel='poly', C=study.best_params['C'], gamma=study.best_params['gamma'],\n",
        "                  epsilon=study.best_params['epsilon'], degree=study.best_params['degree'],\n",
        "                  coef0=study.best_params['coef0'])\n",
        "  best_model.fit(X_train, y_train)\n",
        "  #joblib.dump(best_model, 'PPG_Poly.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8uNS5w0gAHH"
      },
      "outputs": [],
      "source": [
        "def RF_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def RF_objective(trial, cv_runs, X_train, y_train):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 100)#300)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 50) #50\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 10, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features, random_state=28, n_jobs=-1)\n",
        "\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    mean_score = np.mean(scores)\n",
        "    return -mean_score\n",
        "\n",
        "  study = optuna.create_study(direction='minimize')\n",
        "  study.optimize(lambda trial: RF_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "  best_model = RandomForestRegressor(\n",
        "      n_estimators=study.best_params['n_estimators'], max_depth=study.best_params['max_depth'],\n",
        "      min_samples_split=study.best_params['min_samples_split'], min_samples_leaf=study.best_params['min_samples_leaf'],\n",
        "      max_features=study.best_params['max_features'], random_state=28, n_jobs=-1)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  joblib.dump(best_model, 'oPPG_RF.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4dHDJtOsKU_"
      },
      "source": [
        "# **ML Algorithms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLVl7ooxvt7q"
      },
      "outputs": [],
      "source": [
        "def RLE_Model(xTrain, xTest, yTrain, yTest, choice, predict_df, tar_sca): #Function to run Ridge, Lasso, or ElasticNet model\n",
        "  if(choice==\"Ridge\"):\n",
        "    pipeline = Ridge_tune(xTrain, yTrain, 10, 500, 15)\n",
        "\n",
        "  if(choice==\"Lasso\"):\n",
        "    pipeline = Lasso_tune(xTrain, yTrain, 10, 500, 15)\n",
        "\n",
        "  if(choice==\"Elastic\"):\n",
        "    pipeline = Elastic_tune(xTrain, yTrain, 10, 500, 15)\n",
        "\n",
        "  modelResults = Predict_Scores(pipeline, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  #print(f'Chosen alpha  {pipeline.steps[0][1].alpha_:.6f}')\n",
        "  #print(f'Intercept (b) {pipeline.steps[0][1].intercept_:.6f}')\n",
        "  #print(pd.Series(pipeline.steps[0][1].coef_, index=X.columns),'\\n')\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FpU87Mgvvx0s"
      },
      "outputs": [],
      "source": [
        "def GBR_model(xTrain, xTest, yTrain, yTest, predict_df, tar_sca):\n",
        "  model = GBR_tune(xTrain, yTrain, 3, 500, 15)\n",
        "  modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppib1skHfgGk"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "def BR_model(xTrain, xTest, yTrain, yTest, predict_df, tar_sca):\n",
        "  model = BR_tune(xTrain, yTrain, 3, 500, 15)\n",
        "  modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luksQpOa9mx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "def SVR_model(xTrain, xTest, yTrain, yTest, predict_df, tar_sca):\n",
        "  model = SVR_tune(xTrain, yTrain, 10, 500, 15)\n",
        "  modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ms2SSQ_3ncH"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "def SVM_models(xTrain, xTest, yTrain, yTest, choice, predict_df, tar_sca):\n",
        "  if(choice==\"rbf\"):\n",
        "    model = SVR_rbf_tune(xTrain, yTrain, 10, 500, 15)\n",
        "    modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "\n",
        "  if(choice==\"poly\"):\n",
        "    model = SVR_poly_tune(xTrain, yTrain, 3, 500, 15)\n",
        "    modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "\n",
        "  modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfB2M7eKfne7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "def RF_model(xTrain, xTest, yTrain, yTest, predict_df, tar_sca):\n",
        "  model = RF_tune(xTrain, yTrain, 10, 500, 15)\n",
        "  modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4QbyjhZi-0Z"
      },
      "source": [
        "# **Inputs/LOOCV Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8haqjJB_Vjz"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "def corr_matrix_reduce(x_train, x_test):\n",
        "  def compute_corr_row(i, data):\n",
        "      return [data.iloc[:, i].corr(data.iloc[:, j]) for j in range(data.shape[1])]\n",
        "\n",
        "  correlation_matrix = Parallel(n_jobs=-1)(\n",
        "      delayed(compute_corr_row)(i, x_train) for i in range(x_train.shape[1])\n",
        "  )\n",
        "\n",
        "  correlation_matrix = pd.DataFrame(correlation_matrix, columns=x_train.columns, index=x_train.columns)\n",
        "\n",
        "  # Step 2: Reduce features based on correlation threshold\n",
        "  def reduce_features(corr_matrix, threshold=0.9):\n",
        "    #Reduce features by removing one feature from any pair with a correlation above the threshold.\n",
        "      to_drop = set()\n",
        "      for i in range(corr_matrix.shape[0]):\n",
        "          for j in range(i + 1, corr_matrix.shape[1]):\n",
        "              if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                  # Add the second feature to the drop list\n",
        "                  to_drop.add(corr_matrix.columns[j])\n",
        "      return to_drop\n",
        "\n",
        "  threshold = 0.9\n",
        "  features_to_drop = reduce_features(correlation_matrix, threshold)\n",
        "\n",
        "  # Drop the features from the original dataset\n",
        "  x_train_reduced = x_train.drop(columns=features_to_drop)\n",
        "  if x_test.empty != True:\n",
        "    x_test = x_test.drop(columns=features_to_drop)\n",
        "\n",
        "  # Step 3: Print results\n",
        "  print(\"Original features:\", x_train.shape[1])\n",
        "  print(\"Features to drop:\", len(features_to_drop))\n",
        "  print(\"Reduced features:\", x_train_reduced.shape[1])\n",
        "  return x_train_reduced, x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3gOTUsp3kQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "def reduce_df(x_tr, x_te, y_tr, reduction_choice, if_final):\n",
        "  if reduction_choice == \"PLS\":\n",
        "    pls = PLSRegression(n_components=3)\n",
        "    X_tr_pls = pls.fit_transform(x_tr, y_tr)[0]  #Extract transformed features\n",
        "    if x_te.empty:\n",
        "      X_te_pls = pd.DataFrame(columns=[\"PLS1\", \"PLS2\", \"PLS3\"])  #Create empty DataFrame with correct columns if x_te is empty\n",
        "    else:\n",
        "      X_te_pls = pls.transform(x_te)\n",
        "    X_tr_pls = pd.DataFrame(X_tr_pls, columns=[\"PLS1\", \"PLS2\", \"PLS3\"])\n",
        "    X_te_pls = pd.DataFrame(X_te_pls, columns=[\"PLS1\", \"PLS2\", \"PLS3\"])\n",
        "    #print(\"Explained variance in X:\", np.round(pls.x_scores_.var(axis=0) / x_tr.var(axis=0).sum(), 3))\n",
        "    #print(\"Explained variance in Y:\", np.round(pls.y_scores_.var(axis=0) / y_tr.var(), 3))\n",
        "    return X_tr_pls, X_te_pls, pls\n",
        "\n",
        "  if reduction_choice == \"PCA\":\n",
        "    pca=PCA(n_components = 3, random_state=28) #n_components = None, 420\n",
        "    X_tr_PCA = pca.fit_transform(x_tr)\n",
        "    if if_final == \"no\":\n",
        "      X_te_PCA = pca.transform(x_te)\n",
        "    else:\n",
        "      X_te_PCA = x_te\n",
        "    #print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "    #print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "    print(\"Mean:\", pca.mean_)\n",
        "    return X_tr_PCA, X_te_PCA, pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Wz_o_8rgX0"
      },
      "outputs": [],
      "source": [
        "def get_inputs(data_frame, y, tr_index, te_index, scaler_choice, thresh, if_final):\n",
        "#Feature Importance:\n",
        "  if scaler_choice == \"MMS\":\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler2 = MinMaxScaler()\n",
        "  else:\n",
        "    scaler = StandardScaler()\n",
        "    scaler2 = StandardScaler()\n",
        "\n",
        "  if if_final == 'yes':\n",
        "    data_scaled_train, data_scaled_test, y_train, y_test = data_frame, pd.DataFrame(), y, pd.DataFrame()\n",
        "  else:\n",
        "    data_scaled_train, data_scaled_test, y_train, y_test = data_frame.iloc[tr_index], data_frame.iloc[[te_index]], y.iloc[tr_index], y.iloc[te_index]\n",
        "\n",
        "  train_scaled = pd.DataFrame(scaler.fit_transform(data_scaled_train), columns = data_frame.columns)\n",
        "  SOS_train_scaled = train_scaled.mul(train_scaled[\"SOS\"], axis=0)\n",
        "  SOS_train_scaled = SOS_train_scaled.add_suffix('*SOS')\n",
        "  train_scaled = pd.concat([train_scaled, SOS_train_scaled], axis=1)\n",
        "  y_train = pd.Series(scaler2.fit_transform(y_train.values.reshape(-1, 1)).flatten())\n",
        "  train_scaled_correlated, correlations_df = correlation(train_scaled, thresh, y_train) #\n",
        "  train_scaled_correlated = pd.DataFrame(train_scaled_correlated)\n",
        "\n",
        "  # prev_train_scaled_correlated = train_scaled_correlated\n",
        "  # most_corr = correlations_df.columns[0]\n",
        "  # second_most_corr = correlations_df.columns[1]\n",
        "  # third_most_corr = correlations_df.columns[2]\n",
        "  # most_corr_train = prev_train_scaled_correlated.mul(train_scaled_correlated[most_corr], axis=0)\n",
        "  # most_corr_train = most_corr_train.add_suffix(\"*\")\n",
        "  # most_corr_train = most_corr_train.add_suffix(most_corr)\n",
        "  # train_scaled_correlated = pd.concat([train_scaled_correlated, most_corr_train], axis=1)\n",
        "\n",
        "  # second_most_corr_train = prev_train_scaled_correlated.mul(train_scaled_correlated[second_most_corr], axis=0)\n",
        "  # second_most_corr_train = second_most_corr_train.add_suffix(\"*\")\n",
        "  # second_most_corr_train = second_most_corr_train.add_suffix(second_most_corr)\n",
        "  # train_scaled_correlated = pd.concat([train_scaled_correlated, second_most_corr_train], axis=1)\n",
        "\n",
        "  train_scaled_correlated, correlations_df = correlation(train_scaled_correlated, thresh, y_train)\n",
        "\n",
        "  if if_final == 'no':\n",
        "    y_test = pd.Series(y_test)\n",
        "    y_test = y_test.values.reshape(-1, 1)\n",
        "    y_test = scaler2.transform(y_test).flatten()\n",
        "    test_scaled = pd.DataFrame(scaler.transform(data_scaled_test), columns=data_frame.columns)\n",
        "    SOS_test_scaled = test_scaled.mul(test_scaled[\"SOS\"], axis=0)\n",
        "    SOS_test_scaled = SOS_test_scaled.add_suffix('*SOS')\n",
        "    test_scaled = pd.concat([test_scaled, SOS_test_scaled], axis=1)\n",
        "\n",
        "    # prev_test_scaled = test_scaled\n",
        "    # most_corr_test = prev_test_scaled.mul(test_scaled[most_corr], axis=0)\n",
        "    # most_corr_test = most_corr_test.add_suffix(\"*\")\n",
        "    # most_corr_test = most_corr_test.add_suffix(most_corr)\n",
        "    # test_scaled = pd.concat([test_scaled, most_corr_test], axis=1)\n",
        "\n",
        "    # second_most_corr_test = prev_test_scaled.mul(test_scaled[second_most_corr], axis=0)\n",
        "    # second_most_corr_test = second_most_corr_test.add_suffix(\"*\")\n",
        "    # second_most_corr_test = second_most_corr_test.add_suffix(second_most_corr)\n",
        "    # test_scaled = pd.concat([test_scaled, second_most_corr_test], axis=1)\n",
        "\n",
        "    test_scaled_correlated = test_scaled.loc[:, train_scaled_correlated.columns] #Test data with only correlated inputs\n",
        "  else:\n",
        "    test_scaled_correlated = data_scaled_test\n",
        "\n",
        "  train_scaled_correlated, test_scaled_correlated = corr_matrix_reduce(train_scaled_correlated, test_scaled_correlated)\n",
        "  correlations_df2 = correlations_df.loc[:, train_scaled_correlated.columns]\n",
        "\n",
        "  return train_scaled_correlated, test_scaled_correlated, scaler, scaler2, y_train, y_test, correlations_df#,correlations_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIknVAOovdYx"
      },
      "outputs": [],
      "source": [
        "def reduce_and_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, reduction_choice, scaler_target, is_final_model, model_choice):\n",
        "  if reduction_choice == 'PLS':\n",
        "    X_tr_reduced, X_te_reduced, PLS_reducer = reduce_df(X_tr_reduced, X_te_reduced, Y_tr, \"PLS\", is_final_model)\n",
        "  elif reduction_choice == 'PCA':\n",
        "    X_tr_reduced, X_te_reduced, PCA_reducer = reduce_df(X_tr_reduced, X_te_reduced, Y_tr, \"PCA\", is_final_model)\n",
        "\n",
        "  if model_choice == 'Ridge':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Ridge\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'Lasso':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Lasso\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'Elastic':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Elastic\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'GBR':\n",
        "    model = GBR_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'BR':\n",
        "    model = BR_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'SVR':\n",
        "    model = SVR_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'rbf':\n",
        "    model = SVM_models(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"rbf\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'poly':\n",
        "    model = SVM_models(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"poly\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'RF':\n",
        "    model = RF_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sZZxhEdi9Z4w"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "# Define the function that processes each fold of LOO-CV and can make final model\n",
        "def process_fold(train_index, test_index, X, y, reduce_choice, corr_thresh, scaling_choice, modeling_choice):\n",
        "  if scaling_choice == \"MMS\":\n",
        "    X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", corr_thresh, 'no')\n",
        "  else:\n",
        "    X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"\", corr_thresh, 'no')\n",
        "\n",
        "  common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "  if not common_columns:\n",
        "      # Handle the case where there are no correlated columns\n",
        "      return None\n",
        "  X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "  X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "  X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "  X_train_with_corrs = X_train_with_corrs.head(33)\n",
        "  X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "  X_train_reduced = X_train_reduced.transpose()\n",
        "  X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "  X_test_reduced = pd.DataFrame(X_test, columns=X_train_reduced.columns)\n",
        "\n",
        "  model = reduce_and_model(X_train_reduced, X_test_reduced, Y_train, Y_test, reduce_choice, scalerY, 'no', modeling_choice)\n",
        "\n",
        "  return model #Return the model for each fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWimHdH6khJZ"
      },
      "source": [
        "# **Test 1 Fold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAYkKsONkguW"
      },
      "outputs": [],
      "source": [
        "X = inputs\n",
        "y = playoff_stats['oPts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df-1))\n",
        "test_index = list(range((len_df-1), len_df))\n",
        "X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.35, 'no')\n",
        "\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(33)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "X_test_reduced = pd.DataFrame(X_test, columns=X_train_reduced.columns)\n",
        "\n",
        "model = reduce_and_model(X_train_reduced, X_test_reduced, Y_train, Y_test, '', scalerY, 'no', 'Lasso')\n",
        "print(model[4]) #test error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u31AezHyde0V"
      },
      "source": [
        "**see how many cols final model would have:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GjcnyyuFcSuc"
      },
      "outputs": [],
      "source": [
        "X = inputs\n",
        "y = playoff_stats['oPts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df))\n",
        "test_index = list(range(1))\n",
        "\n",
        "X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.35, 'yes')\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(33)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "X_test_reduced = pd.DataFrame(X_test, columns=X_train_reduced.columns)\n",
        "print(len(X_train_reduced.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gimKSs5rrWWW"
      },
      "source": [
        "# **Run LOO-CV**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOgIPBpHZHFg"
      },
      "source": [
        "*   Each scaler for PLS, PCA, and corr\n",
        "*   adjust tuning, only take linear corr columns, mult by top 3 most corr; run longer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baeBkT2AohI6"
      },
      "outputs": [],
      "source": [
        "X = inputs\n",
        "y = playoff_stats['oPts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs\n",
        "loo = LeaveOneOut()\n",
        "results = Parallel(n_jobs=-1)(delayed(process_fold)(train_idx, test_idx, X, y, '', .35, 'MMS', 'Lasso')\n",
        "                              for train_idx, test_idx in loo.split(X))\n",
        "train_NRMSE_scores = 0\n",
        "train_NMAE_scores = 0\n",
        "test_error = 0\n",
        "test_RMSE_num_error = 0\n",
        "train_inv_error = 0\n",
        "test_inv_error = 0\n",
        "len_df = len(X)\n",
        "scaled_y_test_values = []\n",
        "y_test_pred_inv = []\n",
        "\n",
        "for i in range(len_df):\n",
        "  train_NRMSE_scores = results[i][0] + train_NRMSE_scores\n",
        "  train_NMAE_scores = results[i][1] + train_NMAE_scores\n",
        "  test_error = abs(results[i][2]) + test_error\n",
        "  test_RMSE_num_error = results[i][2]**2 + test_RMSE_num_error\n",
        "  train_inv_error = abs(results[i][3]) + train_inv_error\n",
        "  test_inv_error = abs(results[i][4]) + test_inv_error\n",
        "  scaled_y_test_values.append(results[i][5])\n",
        "  y_test_pred_inv.append(results[i][6])\n",
        "\n",
        "range_test_y_scaled = max(scaled_y_test_values) - min(scaled_y_test_values)\n",
        "test_MAE = test_error/len_df\n",
        "test_RMSE = math.sqrt(test_RMSE_num_error[0]/len_df)\n",
        "test_NRMSE = test_RMSE_num_error/range_test_y_scaled\n",
        "test_NMAE = test_MAE/range_test_y_scaled\n",
        "range_preds_inv_transformed = max(y_test_pred_inv) - min(y_test_pred_inv)\n",
        "\n",
        "print(f'AVG training Normalized RMSE: {(train_NRMSE_scores/len_df):.2f}')\n",
        "print(f'AVG training Normalized MAE: {(train_NMAE_scores/len_df):.2f}')\n",
        "print(f'Test Normalized RMSE: {test_NRMSE.item():.2f}')\n",
        "print(f'Test Normalized MAE: {test_NMAE.item():.2f}')\n",
        "print(f'AVG of avg inv transformed train error from folds: {(np.mean(train_inv_error)/len_df):.1f}')\n",
        "print(f'AVG inv transformed test error: {(test_inv_error.item()/len_df):.1f}')\n",
        "print(f'Range of predictions (inv transformed): {(range_preds_inv_transformed.item()):.1f}') #make sure not predicting same value for all preds\n",
        "print('Test Predictions (inv transformed):')\n",
        "for value in y_test_pred_inv:\n",
        "    print(f'{value.item():.0f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "range_target = playoff_stats['oPts/GM_Playoffs'].max() - playoff_stats['oPts/GM_Playoffs'].min()\n",
        "print(f'Range for target: {range_target:.1f}')\n",
        "print(playoff_stats['oPts/GM_Playoffs'])"
      ],
      "metadata": {
        "id": "fmkgLwhdFPi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSUQdmSRnKO8"
      },
      "source": [
        "# **Final Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = inputs\n",
        "y = playoff_stats['oPts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df))\n",
        "test_index = list(range(1))\n",
        "\n",
        "X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.35, 'yes')\n",
        "\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(33)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "\n",
        "model = reduce_and_model(X_train_reduced, X_test, Y_train, Y_test, '', scalerY, 'yes', 'Lasso')\n",
        "\n",
        "range_preds_inv_transformed =  model[7].max() - model[7].min()\n",
        "print(f'Normalized RMSE: {(model[0]):.3f}')\n",
        "print(f'Normalized MAE: {(model[1]):.3f}')\n",
        "print(f'avg inv transformed accuracy: {(np.mean(model[3])):.1f}')\n",
        "print(f'Range of predictions (inv transformed): {(range_preds_inv_transformed):.1f}') #make sure not predicting same value for all preds\n",
        "print('inv transformed predictions:')\n",
        "for value in model[7]:\n",
        "    print(f'{value:.0f}')"
      ],
      "metadata": {
        "id": "jYtJ5AylwvFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81Jm8_OhFIsD"
      },
      "outputs": [],
      "source": [
        "# import scipy.stats\n",
        "# X = inputs\n",
        "# y = playoff_stats['Pts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs #\n",
        "# col = ''\n",
        "# print(X.loc[:, col].corr(y))\n",
        "# print(scipy.stats.spearmanr(X.loc[:,col], y)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **oPPG**"
      ],
      "metadata": {
        "id": "7Cmr94bq3kGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RF MMS - .35**\n",
        "* AVG training Normalized RMSE: 0.20\n",
        "* AVG training Normalized MAE: 0.17\n",
        "* Test Normalized RMSE: 3.01\n",
        "* Test Normalized MAE: 0.26\n",
        "* AVG of avg inv transformed train error from folds: 2.8\n",
        "* AVG inv transformed test error: 4.6\n",
        "* Range of predictions (inv transformed): 5.9\n",
        "\n",
        "**RF MMS Final** - .35\n",
        "* Best parameters: {'n_estimators': 67, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}\n",
        "* Normalized RMSE: 0.192\n",
        "* Normalized MAE: 0.159\n",
        "* avg inv transformed accuracy: 2.6\n",
        "* Range of predictions (inv transformed): 7.1"
      ],
      "metadata": {
        "id": "3g9FN5Ts8NE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oPPG_predict = pd.read_csv('oPPG Inputs_MM2025.csv')\n",
        "X = inputs\n",
        "y = playoff_stats['oPts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs"
      ],
      "metadata": {
        "id": "pcyyQdUHWf6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oPPG_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "trained_features_to_scale = X[oPPG_predict.columns]\n",
        "trained_features_scaled = pd.DataFrame(oPPG_scaler.fit_transform(trained_features_to_scale), columns = trained_features_to_scale.columns)\n",
        "\n",
        "oPPG_predict_scaled = pd.DataFrame(oPPG_scaler.transform(oPPG_predict), columns = oPPG_predict.columns)\n",
        "oPPG_playoffs_scaled = pd.Series(target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten())"
      ],
      "metadata": {
        "id": "_J8WEMLllfYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oPPG_model = joblib.load('oPPG_RF.pkl')\n",
        "\n",
        "trained_columns = oPPG_model.feature_names_in_\n",
        "oPPG_predict_scaled = oPPG_predict_scaled[trained_columns]\n",
        "oPPG_predictions = oPPG_model.predict(oPPG_predict_scaled)\n",
        "\n",
        "oPPG_predictions = target_scaler.inverse_transform(oPPG_predictions.reshape(-1, 1))\n",
        "oPPG_predictions = pd.Series(oPPG_predictions.flatten())\n",
        "oPPG_predictions.to_csv('oPPG_preds.csv')"
      ],
      "metadata": {
        "id": "9slozhx0nIDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEgEYO_sTh3t"
      },
      "source": [
        "# **PPG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlyPEa1BW0tD"
      },
      "source": [
        "**Ridge PCA MMS:** - .5,\n",
        "* AVG training Normalized RMSE: 0.17\n",
        "* AVG training Normalized MAE: 0.14\n",
        "* Test Normalized RMSE: 1.19\n",
        "* Test Normalized MAE: 0.16\n",
        "* AVG of avg inv transformed train error from folds: 3.82\n",
        "* AVG inv transformed test error: 4.7\n",
        "\n",
        "**Ridge Final**\n",
        "* Normalized RMSE: 0.174\n",
        "* Normalized MAE: 0.146\n",
        "* avg inv transformed accuracy: 3.9\n",
        "* Range of predictions (inv transformed): 14.2\n",
        "* Best trial: Params: {'alpha': 5.00004484324578, 'solver': 'svd'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_predict = pd.read_csv('PPG Inputs MM2025.csv')\n",
        "X = inputs\n",
        "y = playoff_stats['Pts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs"
      ],
      "metadata": {
        "id": "K3ZyxUbZxvcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "trained_features_to_scale = X[PPG_predict.columns]\n",
        "trained_features_scaled = pd.DataFrame(PPG_scaler.fit_transform(trained_features_to_scale), columns = trained_features_to_scale.columns)\n",
        "\n",
        "PPG_predict_scaled = pd.DataFrame(PPG_scaler.transform(PPG_predict), columns = PPG_predict.columns)\n",
        "PPG_playoffs_scaled = pd.Series(target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten())"
      ],
      "metadata": {
        "id": "TrvJZ57YxyyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_model = joblib.load('PPG_Ridge.pkl')\n",
        "\n",
        "pca=PCA(n_components = 3, random_state=28)\n",
        "X_train_PCA = pca.fit_transform(X_train_reduced)\n",
        "PPG_predict_scaled = PPG_predict_scaled[X_train_reduced.columns]\n",
        "PPG_predict_scaled_PCA = pca.transform(PPG_predict_scaled)\n",
        "\n",
        "PPG_predictions = PPG_model.predict(PPG_predict_scaled_PCA)\n",
        "PPG_predictions = target_scaler.inverse_transform(PPG_predictions.reshape(-1, 1))\n",
        "PPG_predictions = pd.Series(PPG_predictions.flatten())\n",
        "PPG_predictions.to_csv('PPG_preds.csv')"
      ],
      "metadata": {
        "id": "8aV6d56f1EJ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Vm8WmlMWl5qPkDh4nqbzn64vOM6ndqxq",
      "authorship_tag": "ABX9TyOcG94ZK/J6nDtm7Q8QbTAn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}