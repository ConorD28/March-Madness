{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConorD28/March-Madness/blob/main/March_Madness_B4_Chip_2025_GitHub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq6xsPJUvkXO",
        "outputId": "b70f7089-686a-4733-d81e-6d276dabb74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "inputs = pd.read_csv('MM_b4_Ch_Upload.csv')\n",
        "playoff_stats = inputs.iloc[:, -4:]\n",
        "inputs = inputs.iloc[:, :-4]\n",
        "\n",
        "print(inputs.isnull().sum().sum()) #Check if there are NA values\n",
        "print(playoff_stats.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9zr28DwjQ6D"
      },
      "source": [
        "\n",
        "\n",
        "# **Correlation/Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2LtJLaEVvqfY"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  cols = []\n",
        "  correlations = []\n",
        "  #corS = 0\n",
        "  if isinstance(target, np.ndarray):\n",
        "    target = pd.Series(target)\n",
        "  for col in dataset.columns:\n",
        "      #print(dataset.loc[:,col])\n",
        "      #print(col)\n",
        "      corS = dataset.loc[:,col].corr(target, method='spearman') # 'kendall'\n",
        "      corP = dataset.loc[:,col].corr(target)\n",
        "      if (abs(corP) > threshold) or (abs(corS) > threshold):\n",
        "        cor2 = max(abs(corP), abs(corS))\n",
        "        data.append(dataset.loc[:,col]) #make list of columns that meet the threshold\n",
        "        cols.append(col)\n",
        "        correlations.append(cor2) #make list of correlations that meet the threshold\n",
        "  if len(data) == 0:\n",
        "     return pd.DataFrame()\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "  df_len = len(df.columns)\n",
        "  df.insert(df_len, 'corrs', correlations)\n",
        "  df = df.sort_values(by=df.columns[-1], ascending=False, key = abs)\n",
        "  df = df.transpose()\n",
        "  df_corrs = df.iloc[-1:, :]\n",
        "  df = df.drop(df.tail(1).index)\n",
        "  return df, df_corrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngjdaqWIvssV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random.mtrand import random_sample\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, MultiTaskLassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5q3JY8L1e1M"
      },
      "outputs": [],
      "source": [
        "def Scores(y, y_pred):\n",
        "  MSE = mean_squared_error(y, y_pred)\n",
        "  MAE = mean_absolute_error(y, y_pred)\n",
        "\n",
        "  range_y = y.max() - y.min()\n",
        "  Normalized_RMSE = (np.sqrt(MSE)/abs(range_y))\n",
        "  Normalized_MAE = (MAE/abs(range_y))\n",
        "  #print(f'Normalized RMSE:{ Normalized_RMSE:.2f}')\n",
        "  #print(f'Normalized MAE:{ Normalized_MAE:.2f}')\n",
        "  #print(f'MAE:{ MAE:.3f}')\n",
        "  #print(f'RMSE:{ np.sqrt(MSE):.3f}')\n",
        "  return Normalized_RMSE, Normalized_MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifryB2OHrVml"
      },
      "outputs": [],
      "source": [
        "def Predict_Scores(model, X_tr, X_te, y_tr, y_te, t_sc):#, predict_df):\n",
        "  y_train_pred = model.predict(X_tr)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  if len(y_te) != 0:\n",
        "    y_test_pred = model.predict(X_te)\n",
        "  else:\n",
        "    y_test_pred = pd.DataFrame()\n",
        "  #print('y test values:')\n",
        "  #print(y_test_pred)\n",
        "\n",
        "  #print('Training Scores:')\n",
        "  NRMSE_tr, NMAE_tr = Scores(y_tr, y_train_pred)\n",
        "\n",
        "  #print('after inverse transform, training off by:')\n",
        "  y_train_pred_transformed = t_sc.inverse_transform(y_train_pred.reshape(-1, 1)) # Reshape y_train_pred\n",
        "  y_train_pred_transformed = pd.Series(y_train_pred_transformed.flatten())\n",
        "  y_tr_transformed = t_sc.inverse_transform(y_tr.values.reshape(-1, 1))\n",
        "  y_tr_transformed = pd.Series(y_tr_transformed.flatten())\n",
        "  inv_error_tr_transformed = np.abs(y_tr_transformed - y_train_pred_transformed)\n",
        "  #print('y training values:')\n",
        "  #print(y_train_pred_transformed)\n",
        "\n",
        "  error_test = 0\n",
        "  inv_error_test_transformed = 0\n",
        "  y_te_transformed = 0\n",
        "  y_test_pred_transformed = 0\n",
        "  #Test Predictions:\n",
        "  if len(y_te) != 0:\n",
        "    error_test = y_te - y_test_pred\n",
        "    y_te_transformed = t_sc.inverse_transform(y_te.reshape(-1, 1))\n",
        "    y_te_transformed = y_te_transformed.flatten()\n",
        "    y_test_pred_transformed = t_sc.inverse_transform(y_test_pred.reshape(-1, 1))\n",
        "    y_test_pred_transformed = y_test_pred_transformed.flatten()\n",
        "    y_te_transformed = t_sc.inverse_transform(y_te.reshape(-1, 1))\n",
        "    inv_error_test_transformed = np.abs(y_te_transformed - y_test_pred_transformed)\n",
        "    #print(y_test_pred_transformed)\n",
        "\n",
        "  #Predict:\n",
        "  #predictions = model.predict(predict_df)\n",
        "\n",
        "  return NRMSE_tr, NMAE_tr, error_test, inv_error_tr_transformed, inv_error_test_transformed, y_te, y_test_pred_transformed, y_train_pred_transformed#, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4QbyjhZi-0Z"
      },
      "source": [
        "# **Inputs/LOOCV Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8haqjJB_Vjz"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "def corr_matrix_reduce(x_train, x_test):\n",
        "  def compute_corr_row(i, data):\n",
        "      return [data.iloc[:, i].corr(data.iloc[:, j]) for j in range(data.shape[1])]\n",
        "\n",
        "  correlation_matrix = Parallel(n_jobs=-1)(\n",
        "      delayed(compute_corr_row)(i, x_train) for i in range(x_train.shape[1])\n",
        "  )\n",
        "\n",
        "  correlation_matrix = pd.DataFrame(correlation_matrix, columns=x_train.columns, index=x_train.columns)\n",
        "\n",
        "  # Step 2: Reduce features based on correlation threshold\n",
        "  def reduce_features(corr_matrix, threshold=0.9):\n",
        "    #Reduce features by removing one feature from any pair with a correlation above the threshold.\n",
        "      to_drop = set()\n",
        "      for i in range(corr_matrix.shape[0]):\n",
        "          for j in range(i + 1, corr_matrix.shape[1]):\n",
        "              if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                  # Add the second feature to the drop list\n",
        "                  to_drop.add(corr_matrix.columns[j])\n",
        "      return to_drop\n",
        "\n",
        "  threshold = 0.9\n",
        "  features_to_drop = reduce_features(correlation_matrix, threshold)\n",
        "\n",
        "  # Drop the features from the original dataset\n",
        "  x_train_reduced = x_train.drop(columns=features_to_drop)\n",
        "  if x_test.empty != True:\n",
        "    x_test = x_test.drop(columns=features_to_drop)\n",
        "\n",
        "  # Step 3: Print results\n",
        "  print(\"Original features:\", x_train.shape[1])\n",
        "  print(\"Features to drop:\", len(features_to_drop))\n",
        "  print(\"Reduced features:\", x_train_reduced.shape[1])\n",
        "  return x_train_reduced, x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3gOTUsp3kQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "def reduce_df(x_tr, x_te, y_tr, reduction_choice, if_final):\n",
        "  if reduction_choice == \"PLS\":\n",
        "    pls = PLSRegression(n_components=3)\n",
        "    X_tr_pls = pls.fit_transform(x_tr, y_tr)[0]  #Extract transformed features\n",
        "    if x_te.empty:\n",
        "      X_te_pls = pd.DataFrame(columns=[\"PLS1\", \"PLS2\", \"PLS3\"])  #Create empty DataFrame with correct columns if x_te is empty\n",
        "    else:\n",
        "      X_te_pls = pls.transform(x_te)\n",
        "    X_tr_pls = pd.DataFrame(X_tr_pls, columns=[\"PLS1\", \"PLS2\", \"PLS3\"])\n",
        "    X_te_pls = pd.DataFrame(X_te_pls, columns=[\"PLS1\", \"PLS2\", \"PLS3\"])\n",
        "    #print(\"Explained variance in X:\", np.round(pls.x_scores_.var(axis=0) / x_tr.var(axis=0).sum(), 3))\n",
        "    #print(\"Explained variance in Y:\", np.round(pls.y_scores_.var(axis=0) / y_tr.var(), 3))\n",
        "    return X_tr_pls, X_te_pls, pls\n",
        "\n",
        "  if reduction_choice == \"PCA\":\n",
        "    pca=PCA(n_components = 3, random_state=28) #n_components = None, 420\n",
        "    X_tr_PCA = pca.fit_transform(x_tr)\n",
        "    if if_final == \"no\":\n",
        "      X_te_PCA = pca.transform(x_te)\n",
        "    else:\n",
        "      X_te_PCA = x_te\n",
        "    #print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "    #print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "    print(\"Mean:\", pca.mean_)\n",
        "    return X_tr_PCA, X_te_PCA, pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Wz_o_8rgX0"
      },
      "outputs": [],
      "source": [
        "def get_inputs(data_frame, y, tr_index, te_index, scaler_choice, thresh, if_final):\n",
        "#Feature Importance:\n",
        "  if scaler_choice == \"MMS\":\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler2 = MinMaxScaler()\n",
        "  else:\n",
        "    scaler = StandardScaler()\n",
        "    scaler2 = StandardScaler()\n",
        "\n",
        "  if if_final == 'yes':\n",
        "    data_scaled_train, data_scaled_test, y_train, y_test = data_frame, pd.DataFrame(), y, pd.DataFrame()\n",
        "  else:\n",
        "    data_scaled_train, data_scaled_test, y_train, y_test = data_frame.iloc[tr_index], data_frame.iloc[[te_index]], y.iloc[tr_index], y.iloc[te_index]\n",
        "\n",
        "  train_scaled = pd.DataFrame(scaler.fit_transform(data_scaled_train), columns = data_frame.columns)\n",
        "  SOS_train_scaled = train_scaled.mul(train_scaled[\"SOS\"], axis=0)\n",
        "  SOS_train_scaled = SOS_train_scaled.add_suffix('*SOS')\n",
        "  SOS_Opp_Ch_train_scaled = train_scaled.mul(train_scaled[\"SOS_Opp_Ch\"], axis=0)\n",
        "  SOS_Opp_Ch_train_scaled = SOS_Opp_Ch_train_scaled.add_suffix('*SOS_Opp_Ch')\n",
        "  train_scaled = pd.concat([train_scaled, SOS_train_scaled], axis=1)\n",
        "  train_scaled = pd.concat([train_scaled, SOS_Opp_Ch_train_scaled], axis=1)\n",
        "  y_train = pd.Series(scaler2.fit_transform(y_train.values.reshape(-1, 1)).flatten())\n",
        "  train_scaled_correlated, correlations_df = correlation(train_scaled, thresh, y_train) #\n",
        "  train_scaled_correlated = pd.DataFrame(train_scaled_correlated)\n",
        "\n",
        "  # prev_train_scaled_correlated = train_scaled_correlated\n",
        "  # most_corr = correlations_df.columns[0]\n",
        "  # second_most_corr = correlations_df.columns[1]\n",
        "  # third_most_corr = correlations_df.columns[2]\n",
        "  # most_corr_train = prev_train_scaled_correlated.mul(train_scaled_correlated[most_corr], axis=0)\n",
        "  # most_corr_train = most_corr_train.add_suffix(\"*\")\n",
        "  # most_corr_train = most_corr_train.add_suffix(most_corr)\n",
        "  # train_scaled_correlated = pd.concat([train_scaled_correlated, most_corr_train], axis=1)\n",
        "\n",
        "  # second_most_corr_train = prev_train_scaled_correlated.mul(train_scaled_correlated[second_most_corr], axis=0)\n",
        "  # second_most_corr_train = second_most_corr_train.add_suffix(\"*\")\n",
        "  # second_most_corr_train = second_most_corr_train.add_suffix(second_most_corr)\n",
        "  # train_scaled_correlated = pd.concat([train_scaled_correlated, second_most_corr_train], axis=1)\n",
        "\n",
        "  # third_most_corr_train = prev_train_scaled_correlated.mul(train_scaled_correlated[third_most_corr], axis=0)\n",
        "  # third_most_corr_train = third_most_corr_train.add_suffix(\"*\")\n",
        "  # third_most_corr_train = third_most_corr_train.add_suffix(third_most_corr)\n",
        "  # train_scaled_correlated = pd.concat([train_scaled_correlated, third_most_corr_train], axis=1)\n",
        "  train_scaled_correlated, correlations_df = correlation(train_scaled_correlated, thresh, y_train)\n",
        "\n",
        "  if if_final == 'no':\n",
        "    y_test = pd.Series(y_test)\n",
        "    y_test = y_test.values.reshape(-1, 1)\n",
        "    y_test = scaler2.transform(y_test).flatten()\n",
        "    test_scaled = pd.DataFrame(scaler.transform(data_scaled_test), columns=data_frame.columns)\n",
        "    SOS_test_scaled = test_scaled.mul(test_scaled[\"SOS\"], axis=0)\n",
        "    SOS_test_scaled = SOS_test_scaled.add_suffix('*SOS')\n",
        "    SOS_Opp_Ch_test_scaled = test_scaled.mul(test_scaled[\"SOS_Opp_Ch\"], axis=0)\n",
        "    SOS_Opp_Ch_test_scaled = SOS_Opp_Ch_test_scaled.add_suffix('*SOS_Opp_Ch')\n",
        "    test_scaled = pd.concat([test_scaled, SOS_test_scaled], axis=1)\n",
        "    test_scaled = pd.concat([test_scaled, SOS_Opp_Ch_test_scaled], axis=1)\n",
        "\n",
        "    # prev_test_scaled = test_scaled\n",
        "    # most_corr_test = prev_test_scaled.mul(test_scaled[most_corr], axis=0)\n",
        "    # most_corr_test = most_corr_test.add_suffix(\"*\")\n",
        "    # most_corr_test = most_corr_test.add_suffix(most_corr)\n",
        "    # test_scaled = pd.concat([test_scaled, most_corr_test], axis=1)\n",
        "\n",
        "    # second_most_corr_test = prev_test_scaled.mul(test_scaled[second_most_corr], axis=0)\n",
        "    # second_most_corr_test = second_most_corr_test.add_suffix(\"*\")\n",
        "    # second_most_corr_test = second_most_corr_test.add_suffix(second_most_corr)\n",
        "    # test_scaled = pd.concat([test_scaled, second_most_corr_test], axis=1)\n",
        "\n",
        "    # third_most_corr_test = prev_test_scaled.mul(test_scaled[third_most_corr], axis=0)\n",
        "    # third_most_corr_test = third_most_corr_test.add_suffix(\"*\")\n",
        "    # third_most_corr_test = third_most_corr_test.add_suffix(third_most_corr)\n",
        "    # test_scaled = pd.concat([test_scaled, third_most_corr_test], axis=1)\n",
        "\n",
        "    test_scaled_correlated = test_scaled.loc[:, train_scaled_correlated.columns] #Test data with only correlated inputs\n",
        "  else:\n",
        "    test_scaled_correlated = data_scaled_test\n",
        "\n",
        "  train_scaled_correlated, test_scaled_correlated = corr_matrix_reduce(train_scaled_correlated, test_scaled_correlated)\n",
        "  correlations_df2 = correlations_df.loc[:, train_scaled_correlated.columns]\n",
        "\n",
        "  return train_scaled_correlated, test_scaled_correlated, scaler, scaler2, y_train, y_test, correlations_df#,correlations_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIknVAOovdYx"
      },
      "outputs": [],
      "source": [
        "def reduce_and_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, reduction_choice, scaler_target, is_final_model, model_choice):\n",
        "  if reduction_choice == 'PLS':\n",
        "    X_tr_reduced, X_te_reduced, PLS_reducer = reduce_df(X_tr_reduced, X_te_reduced, Y_tr, \"PLS\", is_final_model)\n",
        "  elif reduction_choice == 'PCA':\n",
        "    X_tr_reduced, X_te_reduced, PCA_reducer = reduce_df(X_tr_reduced, X_te_reduced, Y_tr, \"PCA\", is_final_model)\n",
        "\n",
        "  if model_choice == 'Ridge':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Ridge\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'Lasso':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Lasso\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'Elastic':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Elastic\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'GBR':\n",
        "    model = GBR_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'BR':\n",
        "    model = BR_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'SVR':\n",
        "    model = SVR_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'rbf':\n",
        "    model = SVM_models(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"rbf\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'poly':\n",
        "    model = SVM_models(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"poly\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'RF':\n",
        "    model = RF_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sZZxhEdi9Z4w"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "# Define the function that processes each fold of LOO-CV and can make final model\n",
        "def process_fold(train_index, test_index, X, y, reduce_choice, corr_thresh, scaling_choice, modeling_choice):\n",
        "  if scaling_choice == \"MMS\":\n",
        "    X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", corr_thresh, 'no')\n",
        "  else:\n",
        "    X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"\", corr_thresh, 'no')\n",
        "\n",
        "  common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "  if not common_columns:\n",
        "      # Handle the case where there are no correlated columns\n",
        "      return None\n",
        "  X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "  X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "  X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "  X_train_with_corrs = X_train_with_corrs.head(53)\n",
        "  X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "  X_train_reduced = X_train_reduced.transpose()\n",
        "  X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "  X_test_reduced = pd.DataFrame(X_test, columns=X_train_reduced.columns)\n",
        "\n",
        "  model = reduce_and_model(X_train_reduced, X_test_reduced, Y_train, Y_test, reduce_choice, scalerY, 'no', modeling_choice)\n",
        "\n",
        "  return model #Return the model for each fold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PPG**"
      ],
      "metadata": {
        "id": "0-hA0er28Bok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Folds:**\n",
        "*   AVG training Normalized RMSE: 0.03\n",
        "*   AVG training Normalized MAE: 0.03\n",
        "*   Test Normalized RMSE: 0.44\n",
        "*   Test Normalized MAE: 0.05\n",
        "*   AVG of avg inv transformed train error from folds: 0.7\n",
        "*   AVG inv transformed test error: 1.5\n",
        "*   Range of predictions (inv transformed): 25.2\n",
        "\n",
        "**Lasso Final:** - .49 MMS, 53:\n",
        "*   Best trial:\n",
        "  Params: {'alpha': 0.010000190200792278, 'max_iter': 2600, 'tol': 0.0022029157857933754}\n",
        "*   Normalized RMSE: 0.064\n",
        "*   Normalized MAE: 0.055\n",
        "*   avg inv transformed accuracy: 1.5\n",
        "*   Range of predictions (inv transformed): 21.7"
      ],
      "metadata": {
        "id": "IHySmN3o8ESa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = inputs\n",
        "y = playoff_stats['Pts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df))\n",
        "test_index = list(range(1))\n",
        "\n",
        "X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.49, 'yes')\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(53)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "i9EbC83IC7Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_predict = pd.read_csv('PPG_Inputs_b4_chip1.csv')\n",
        "\n",
        "PPG_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "trained_features_to_scale = X[PPG_predict.columns]\n",
        "trained_features_scaled = pd.DataFrame(PPG_scaler.fit_transform(trained_features_to_scale), columns = trained_features_to_scale.columns)\n",
        "\n",
        "PPG_predict_scaled = pd.DataFrame(PPG_scaler.transform(PPG_predict), columns = PPG_predict.columns)\n",
        "PPG_playoffs_scaled = pd.Series(target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PJI-GMS_DBev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs*SOS'] = PPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Seed_Playoffs*SOS'] = PPG_predict_scaled['Seed_Playoffs'] * PPG_predict_scaled['SOS']\n",
        "PPG_predict_scaled['Seed_Playoffs*SOS_Opp_Ch'] = PPG_predict_scaled['Seed_Playoffs'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Conference Leader PTS/GM - PTS/GM_Off*SOS'] = PPG_predict_scaled['Conference Leader PTS/GM - PTS/GM_Off'] * PPG_predict_scaled['SOS']\n",
        "PPG_predict_scaled['Conference Leader PTS/GM - PTS/GM_Off*SOS_Opp_Ch'] = PPG_predict_scaled['Conference Leader PTS/GM - PTS/GM_Off*SOS']\n",
        "\n",
        "PPG_predict_scaled['National Leader Point Diff - Point Diff_Off*SOS_Opp_Ch'] = PPG_predict_scaled['National Leader Point Diff - Point Diff_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Conference Leader AST/GM - AST/GM_Off*SOS_Opp_Ch'] = PPG_predict_scaled['Conference Leader AST/GM - AST/GM_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Conference Leader AST/GM - AST/GM_Off_Opp_Ch*SOS_Opp_Ch'] = PPG_predict_scaled['Conference Leader AST/GM - AST/GM_Off_Opp_Ch'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['oPts/GM_bfour_Ch_Playoffs*SOS'] = PPG_predict_scaled['oPts/GM_bfour_Ch_Playoffs'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['PTS/GM Nat. Rank * FG% Nat. Rank_Off*SOS_Opp_Ch'] = PPG_predict_scaled['PTS/GM Nat. Rank * FG% Nat. Rank_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['PTS/AST_Off_Opp_Ch*SOS_Opp_Ch'] = PPG_predict_scaled['PTS/AST_Off_Opp_Ch'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled[' PTS/GM Conference Rank_Off*SOS_Opp_Ch'] = PPG_predict_scaled[' PTS/GM Conference Rank_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['PTS/GM*FG%_Off*SOS'] = PPG_predict_scaled['PTS/GM*FG%_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['oPPG/SOS_Opp_1st*SOS'] = PPG_predict_scaled['oPPG/SOS_Opp_1st'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['AST/GM National Rank_Off_Opp_Ch*SOS_Opp_Ch'] = PPG_predict_scaled['AST/GM National Rank_Off_Opp_Ch'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Conference Leader FG% - FG%_Off*SOS_Opp_Ch'] = PPG_predict_scaled['Conference Leader FG% - FG%_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs*SOS'] = PPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs'] * PPG_predict_scaled['SOS']\n",
        "PPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs*SOS_Opp_Ch'] = PPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs*SOS']\n",
        "\n",
        "PPG_predict_scaled['PTS/GM Nat. Rank * 3PM/GM Nat. Rank_Off*SOS_Opp_Ch'] = PPG_predict_scaled['PTS/GM Nat. Rank * 3PM/GM Nat. Rank_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['AST/GM National Rank_Off*SOS_Opp_Ch'] = PPG_predict_scaled['AST/GM National Rank_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled[' AST/GM Conference Rank_Off*SOS_Opp_Ch'] = PPG_predict_scaled[' AST/GM Conference Rank_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['PTS/GM National Rank_Off*SOS_Opp_Ch'] = PPG_predict_scaled['PTS/GM National Rank_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Conference Leader FT% - FT%_Off_Opp_Ch*SOS'] = PPG_predict_scaled['Conference Leader FT% - FT%_Off_Opp_Ch'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['Point Diff/GM_bfour_Ch_Playoffs*SOS'] = PPG_predict_scaled['Point Diff/GM_bfour_Ch_Playoffs'] * PPG_predict_scaled['SOS']\n",
        "PPG_predict_scaled['Point Diff/GM_bfour_Ch_Playoffs*SOS_Opp_Ch'] = PPG_predict_scaled['Point Diff/GM_bfour_Ch_Playoffs'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['FG% National Rank_Off*SOS_Opp_Ch'] = PPG_predict_scaled['FG% National Rank_Off'] * PPG_predict_scaled['SOS']\n",
        "\n",
        "PPG_predict_scaled['2P%*AST/GM_Off_Opp_Ch*SOS'] = PPG_predict_scaled['2P%*AST/GM_Off_Opp_Ch'] * PPG_predict_scaled['SOS']"
      ],
      "metadata": {
        "id": "TRFCgmJEEIvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_model = joblib.load('PPG_Lasso_.pkl')\n",
        "\n",
        "trained_columns = PPG_model.feature_names_in_\n",
        "PPG_predict_scaled = PPG_predict_scaled[trained_columns]\n",
        "PPG_predictions = PPG_model.predict(PPG_predict_scaled)"
      ],
      "metadata": {
        "id": "pDk0XEkiD4Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PPG_predictions = target_scaler.inverse_transform(PPG_predictions.reshape(-1, 1))\n",
        "PPG_predictions = pd.Series(PPG_predictions.flatten())\n",
        "PPG_predictions.values"
      ],
      "metadata": {
        "id": "euiTfHBZD5PO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef534046-7709-4e3e-e2a1-5a06560456ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([81.80213449, 71.77994519])"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **oPPG**"
      ],
      "metadata": {
        "id": "c9XTy8gNnTA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Folds:**\n",
        "*   AVG training Normalized RMSE: 0.09\n",
        "*   AVG training Normalized MAE: 0.07\n",
        "*   Test Normalized RMSE: 0.34\n",
        "*   Test Normalized MAE: 0.08\n",
        "*   AVG of avg inv transformed train error from folds: 1.2\n",
        "*   AVG inv transformed test error: 1.4\n",
        "*   Range of predictions (inv transformed): 13.0\n",
        "\n",
        "**Lasso Final:**\n",
        "*   Best trial:\n",
        "  Params: {'alpha': 0.010001255702060242, 'max_iter': 2700, 'tol': 0.00044933505675711976}\n",
        "*   Normalized RMSE: 0.091\n",
        "*   Normalized MAE: 0.070\n",
        "*   avg inv transformed accuracy: 1.2\n",
        "*   Range of predictions (inv transformed): 13.2"
      ],
      "metadata": {
        "id": "zVqRBC7Vnb7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = inputs\n",
        "y = playoff_stats['oPts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df))\n",
        "test_index = list(range(1))\n",
        "\n",
        "X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.4, 'yes')\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(53)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "Mw_FjziPn_-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oPPG_predict = pd.read_csv('oPPG_Inputs_b4_chip.csv')\n",
        "\n",
        "oPPG_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "trained_features_to_scale = X[oPPG_predict.columns]\n",
        "trained_features_scaled = pd.DataFrame(oPPG_scaler.fit_transform(trained_features_to_scale), columns = trained_features_to_scale.columns)\n",
        "\n",
        "oPPG_predict_scaled = pd.DataFrame(oPPG_scaler.transform(oPPG_predict), columns = oPPG_predict.columns)\n",
        "oPPG_playoffs_scaled = pd.Series(target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten())"
      ],
      "metadata": {
        "id": "xNDNNn_hoJ_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oPPG_predict_scaled['oPts/GM_bfour_Ch_Playoffs*SOS'] = oPPG_predict_scaled['oPts/GM_bfour_Ch_Playoffs'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['oPts/GM_bfour_Ch_Playoffs*SOS_Opp_Ch'] = oPPG_predict_scaled['oPts/GM_bfour_Ch_Playoffs*SOS']\n",
        "\n",
        "oPPG_predict_scaled['National Leader FG% - FG%_Off_Opp_Ch*SOS'] = oPPG_predict_scaled['National Leader FG% - FG%_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['National Leader FG% - FG%_Off_Opp_Ch*SOS_Opp_Ch'] = oPPG_predict_scaled['National Leader FG% - FG%_Off_Opp_Ch*SOS']\n",
        "\n",
        "oPPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs*SOS'] = oPPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs*SOS_Opp_Ch'] = oPPG_predict_scaled['Pts/GM_bfour_Ch_Playoffs*SOS']\n",
        "\n",
        "oPPG_predict_scaled['AST/GM National Rank_Off_Opp_Ch*SOS'] = oPPG_predict_scaled['AST/GM National Rank_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['AST/GM National Rank_Off_Opp_Ch*SOS_Opp_Ch'] = oPPG_predict_scaled['AST/GM National Rank_Off_Opp_Ch*SOS']\n",
        "\n",
        "oPPG_predict_scaled['Conference Leader PTS/GM - PTS/GM_Off*SOS'] = oPPG_predict_scaled['Conference Leader PTS/GM - PTS/GM_Off'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['FGA/AST_Off_Opp_Ch*SOS'] = oPPG_predict_scaled['FGA/AST_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled[' FG% Conference Rank_Off*SOS_Opp_Ch'] = oPPG_predict_scaled[' FG% Conference Rank_Off'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['2PM/GM*2P%_Off_Opp_Ch*SOS'] = oPPG_predict_scaled['2PM/GM*2P%_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['2PM/GM*2P%_Off_Opp_Ch*SOS_Opp_Ch'] = oPPG_predict_scaled['2PM/GM*2P%_Off_Opp_Ch*SOS']\n",
        "\n",
        "oPPG_predict_scaled['STL/GM Conference Rank_Def_Opp_Ch*SOS'] = oPPG_predict_scaled['STL/GM Conference Rank_Def_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['STL/GM Conference Rank_Def_Opp_Ch*SOS_Opp_Ch'] = oPPG_predict_scaled['STL/GM Conference Rank_Def_Opp_Ch*SOS']\n",
        "\n",
        "oPPG_predict_scaled['3P%*AST/3PA_Off_Opp_Ch*SOS'] = oPPG_predict_scaled['3P%*AST/3PA_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['Conference Leader AST/GM - AST/GM_Off_Opp_Ch*SOS'] = oPPG_predict_scaled['Conference Leader AST/GM - AST/GM_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled[' PTS/GM Conference Rank_Off*SOS'] = oPPG_predict_scaled[' PTS/GM Conference Rank_Off'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['PORPAG_BP/Team GM_Opp_Ch*SOS'] = oPPG_predict_scaled['PORPAG_BP/Team GM_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['FG% National Rank_Off_Opp_Ch*SOS'] = oPPG_predict_scaled['FG% National Rank_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['oPTS/TO_Def_Opp_Ch*SOS'] = oPPG_predict_scaled['oPTS/TO_Def_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['oPTS/TO_Def_Opp_Ch*SOS_Opp_Ch'] = oPPG_predict_scaled['oPTS/TO_Def_Opp_Ch*SOS']\n",
        "\n",
        "oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM*SOS'] = oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM*SOS_Opp_Ch'] = oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM*SOS']\n",
        "\n",
        "oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM_Opp_Ch*SOS'] = oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM_Opp_Ch*SOS_Opp_Ch'] = oPPG_predict_scaled['ThreePointFieldGoals Attempted/Team GM_BP/Team GM_Opp_Ch*SOS']\n",
        "\n",
        "oPPG_predict_scaled['AST/FGA * FG%_Off_Opp_Ch*SOS_Opp_Ch'] = oPPG_predict_scaled['AST/FGA * FG%_Off_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['FreeThrows Made/Team GM_BP/Team GM_Opp_Ch*SOS'] = oPPG_predict_scaled['FreeThrows Made/Team GM_BP/Team GM_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['FieldGoals Attempted/Team GM_BP/Team GM_Opp_Ch*SOS'] = oPPG_predict_scaled['FieldGoals Attempted/Team GM_BP/Team GM_Opp_Ch'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['oPTS/oFTM_Def*SOS_Opp_Ch'] = oPPG_predict_scaled['oPTS/oFTM_Def'] * oPPG_predict_scaled['SOS']\n",
        "\n",
        "oPPG_predict_scaled['FieldGoals Pct/MP_BP/MP*SOS'] = oPPG_predict_scaled['FieldGoals Pct/MP_BP/MP'] * oPPG_predict_scaled['SOS']"
      ],
      "metadata": {
        "id": "ibRYFhMcwgHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oPPG_model = joblib.load('oPPG_Lasso_.pkl')\n",
        "\n",
        "trained_columns = oPPG_model.feature_names_in_\n",
        "oPPG_predict_scaled = oPPG_predict_scaled[trained_columns]\n",
        "oPPG_predictions = oPPG_model.predict(oPPG_predict_scaled)"
      ],
      "metadata": {
        "id": "xVdOSCfzqXxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oPPG_predictions = target_scaler.inverse_transform(oPPG_predictions.reshape(-1, 1))\n",
        "oPPG_predictions = pd.Series(oPPG_predictions.flatten())\n",
        "oPPG_predictions.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9QxxPbJqi1V",
        "outputId": "da55fe6b-f66d-4a20-bf06-146fb343bc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([69.47963684, 61.02079017])"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l9zr28DwjQ6D",
        "0-hA0er28Bok",
        "KSUQdmSRnKO8"
      ],
      "provenance": [],
      "mount_file_id": "1Vm8WmlMWl5qPkDh4nqbzn64vOM6ndqxq",
      "authorship_tag": "ABX9TyOr3AlI7WyPkbQAWUTkkvZm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}